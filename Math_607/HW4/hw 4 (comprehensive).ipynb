{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do here\n",
    "\n",
    "I basically said this already in class, but: this is just another assignment; the only difference to the previous assignments is that it is comprehensive in scope, tying together some of the stuff we've discussed over the last term.  You can work together with your classmates, you can look up how to do the problems on the internet, you can ask me, etc.  The only ground rules are:\n",
    "\n",
    "- cite your sources,\n",
    "- say who you worked with,\n",
    "- write the final submission up on your own and submit your own work.\n",
    "\n",
    "**Please write up and submit solutions to four of the following 5 problems.**  Some of the problems do depend on each other, and I hope that you end up working on all of them - but you only need to submit four of them; I don't care which ones. \n",
    "\n",
    "Also: if you are stuck, **please ask me questions**. I'm reserving Thursday Jun 7th's class for initial questions.  You can ask me questions over email or by appointment.  I may hide in my home and do appointments over Zoom, if the grass pollen situation is bad (spoiler: it is bad).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ising model\n",
    "\n",
    "The *Ising model* is a statistical mechanical model; it is an idealized two-dimensional model of a ferromagnet in a magnetic field.  Imagine a 2 dimensional grid of iron atoms in the $xy$ plane, each of which has its north pole pointed up or down in the $z$ direction.  Due to magnetism, the atoms tend to arrange themselves so that they're pointing in opposite directions to their neighbors.  If my north pole is pointed up, then my neighbor's north pole is likely to be pointed down.\n",
    "\n",
    "The state space is $\\{-1, 1\\}^{n^2}$ - that is, at each site $(i,j)$ on an $n \\times n$ square grid, we set $\\sigma_{ij} = \\pm 1$; a state $\\sigma$ consists of these $n^2$ $\\pm 1$'s, which we write as $\\sigma_{ij}$ and call $spins$.  You'd generally say that a 1 is \"spin up\" and a -1 is \"spin down\".\n",
    "\n",
    "\n",
    "The probability of a state $\\sigma$ is defined as follows: first of all, define three constants: \n",
    "- $J$, the \"interaction strength\", can be positive or negative;\n",
    "- $\\mu$, the \"external magnetic field strength\" - again, positive or negative\n",
    "- $\\beta$, the \"inverse temperature\", a positive constant.\n",
    "\n",
    "Define a \"Hamiltonian\"\n",
    "\n",
    "$$H(\\sigma) = \n",
    "-J \\sum_{1 \\leq i \\leq n} \\sum_{1 \\leq j < n} \\sigma_{i,j} \\sigma_{i,j+1}\n",
    "-J \\sum_{1 \\leq i < n} \\sum_{1 \\leq j \\leq n} \\sigma_{i,j} \\sigma_{i+1,j}\n",
    "- \\sum_{1 \\leq i,j \\leq n}\\mu \\sigma_{i,j}\n",
    "$$\n",
    "\n",
    "So, only nearest-neighbor spins interact with each other.\n",
    "\n",
    "Then, the probability of the state $\\sigma$ is proportional to $\\exp( -\\beta H(\\sigma))$.\n",
    "\n",
    "(a) Describe a Metropolis-Hastings markov chain which samples from the Ising model, for various $n$ (I imagine about $n=20$ is the biggest you'll want to go).\n",
    "\n",
    "(b) Implement monotone coupling from the past, and draw some pictures of samples from the ising model.  Try doing $J=1$ and $J=-1$, with various values for $\\beta$ and $\\mu$.  I'd suggest you draw white pixels for -1s and black squares for +1s.  What, qualitatively, do $J$, $\\mu$, $\\beta$ represent?  Explain the results and discuss whether this is a good model for a ferromagnet.\n",
    "\n",
    "(c) When $J = \\pm 1$, $\\beta$ is large (i.e. when the temperature is low) and when $\\mu$ is zero (no external magnetic field), I would imagine that the code in (b) takes many more steps to converge, due to considerations discussed in lecture Monday.  Demonstrate this happening, by plotting the number of steps as a function of $n$. compare \n",
    "\n",
    "\n",
    "There's lots of information on this model available on the internet - maybe too much; there are exact expressions for the partition function, and lots of other information available. I'd recommend you look at Wikipedia for some guidance; my notation is similar to theirs.  Please cite your sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous time markov chains\n",
    "\n",
    "Here we do a thing that I really ought to have done in lecture, but did not: describe *continuous-time Markov chains (with bounded rates)* .  They're just like the discrete-time Markov chains, but the jumps happen at random times.\n",
    "\n",
    "Let $N(t)$ be a Poisson process on $\\mathbb{R}_{\\geq 0}$ with rate $\\lambda$; let $Y_n$ be a (discrete-time) Markov chain with transition matrix $A$.  Set $X_t = Y_{N(t)}$ - so: \n",
    " - $X$ is defined at all times $t \\in \\mathbb{R}_{\\geq 0}$, \n",
    " - $X$ jumps to a new state at the times of the point process $N$.\n",
    " \n",
    "We say that $X$ is a *continuous-time markov chain*.\n",
    "\n",
    "(a) Explain why $X$ has the *continuous-time Markov property*: that is, for times $0 < s_0 < s_1 < ... < s < t$ and states $i_0, \\ldots i_n, i, j$, we have\n",
    "$$ \n",
    "\\mathbb{P}(X_{t+s} = j \\;|\\; X_s = i, X_{s_0} = i_0,  \\ldots X_{s_n} = i_n)\n",
    "=\n",
    "\\mathbb{P}(X_{t} = j \\;|\\; X_0 = i).\n",
    "$$\n",
    "That is, the future is independent of the past, conditioned on the present; moreover, the probability of going from $i$ at time $s$ to $j$ at time $s+t$ depends only on $t$, the difference between the times.\n",
    "\n",
    "(b) Define the transition probability $p_t(i,j) = \\mathbb{P}(X_t = j \\;|\\;X_0 = i)$.  Prove that $\\sum_{k}p_s(i,k)p_t(k,j) = p_{s+t}(i,j).$  \n",
    "\n",
    "(c) Show that the probability that $X$ jumps more than once by time $h$ is $o(h)$.  That is, as h tends to ZERO (not infinity), the probability in question tends to zero.\n",
    "\n",
    "(d) That is, conclude that\n",
    "$$\\frac{d}{dh}p_h(i,j) = \\lim_{h \\rightarrow 0} \\frac{p_h(i,j)}{h} = \\lambda A_{ij}.$$  \n",
    "\n",
    "We call this quantity the *jump rate* from $i$ to $j$.  In applications, your problem generally tells you these jump rates, and then the \"modelling\" step is to work out what $\\lambda$ and the transition probabilities are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of continuous time random walk\n",
    "\n",
    "Carrying on from the above problem: let's simulate a continuous-time random walk $X_t$ on a 5-dimensional hypercube, with constant jump rate $\\lambda = 1$ - this is a graph with vertices $\\{0,1\\}^{5}$, and edges between any two vertices that differ in exactly one coordinate.  Start the walk at the origin.\n",
    "\n",
    "By analogy to the discrete-time situation, the chain converges to the uniform distribution (i.e. $X_t$ is approximately uniformly distributed when $t$ is large enough).  Demonstrate this using simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Normal Fitting\n",
    "\n",
    "(a) Let $L$ be the lower unitriangular $10 \\times 10$ matrix with entries $$L_{ij} = \\frac{1}{i-j+1} \\qquad (i \\geq j)$$ and let $C = LL^T$.  Simulate a sample $S$ of ten thousand points from a multivariate normal distribution with mean 0 and covariance matrix $C$. \n",
    "\n",
    "(b) Now, pretend you forgot all of the above constructions - you have only the sample $S$. Describe some features of $S$ which indicate that it's plausibly drawn from a multivariate normal distribution, and under the assumption that it is, infer what the matrix $L$ was.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brownian Bridge\n",
    "\n",
    "(a) Simulate a Brownian bridge (a brownian motion on [0,1], conditioned to start and end at zero) using Levy's construction (the one with wavelets) - using a resolution small enough to draw some good pictures of the process.  Draw those pictures.  You can take Peter's notes from the Gaussian Processes section as a reference.\n",
    "\n",
    "(b) The brownian bridge is supposed to be an example of a Gaussian process - say what that means.  Give empirical evidence that your process is Gaussian, based on multiple runs of your simulation in (a)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
